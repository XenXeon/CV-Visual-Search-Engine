"""
SVM CLASSIFIER ON PCA FEATURES 

This script trains and evaluates a Support Vector Machine (SVM)
classifier using the pre-computed PCA-transformed feature vectors
generated by 'compute_pca.py'.
"""

## 1. IMPORTS
import cv2
import os
import numpy as np
import scipy.io as sio  # To load .mat descriptor files
from glob import glob   # To find all image files
import time
import random
import matplotlib.pyplot as plt
from tqdm import tqdm   # For a progress bar during feature loading
import seaborn as sns   # <-- ADDED for heatmap plotting

# Scikit-learn Imports
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC  # Support Vector Machine Classifier
from sklearn.preprocessing import StandardScaler  # For feature scaling
from sklearn.pipeline import Pipeline  # To chain scaler and SVM
from sklearn.metrics import (
    accuracy_score, 
    classification_report, 
    ConfusionMatrixDisplay, # This is no longer used but left for reference
    confusion_matrix        # <-- ADDED for heatmap calculation
)

print("Script started... (Loading pre-computed PCA features)")

## 2. CONFIGURATION
# Set these paths manually

# IMPORTANT: This must be the *output folder* from 'compute_pca.py'
# It should contain the .mat files of the PCA-projected descriptors.
PCA_DESCRIPTOR_PATH = r"D:\Surrey\Computer Vision\Skeleton_Python-R1\Skeleton_Python\MSRC_ObjCategImageDatabase_v2\descriptors\lbp4x4_bins10_pca" 

# We still need the *original* image folder to get the filenames
# which we use to determine the class labels (e.g., '1_...bmp' -> label '1')
DATASET_PATH = r"D:\Surrey\Computer Vision\Skeleton_Python-R1\Skeleton_Python\MSRC_ObjCategImageDatabase_v2\Images" 

## 3. LOAD IMAGE PATHS AND LABELS
# We are not loading the *images* here, just their file paths
# and extracting the ground-truth labels from the filenames.

# Get a list of all .bmp files in the image folder
image_paths = glob(os.path.join(DATASET_PATH, "*.bmp"))
labels = []
all_image_paths = []

print("Loading file paths and extracting labels...")
for img_path in image_paths:
    filename = os.path.basename(img_path)  # e.g., '1_1_s.bmp'
    # Assumes filename format 'label_instance_...bmp'
    label = filename.split('_')[0]         # e.g., '1'
    labels.append(label)
    all_image_paths.append(img_path)

print(f"Found {len(all_image_paths)} images with {len(np.unique(labels))} classes.")

## 4. SPLIT INTO TRAINING AND TESTING SETS
# We split the *list of paths and labels* first, *before* loading
# the heavy feature data.
# 'stratify=labels' ensures that the train and test sets have the
# same percentage of samples from each class as the original dataset.
# This is crucial for imbalanced datasets.
paths_train, paths_test, y_train, y_test = train_test_split(
    all_image_paths, 
    labels, 
    test_size=0.2,     # 20% of data for testing, 80% for training
    random_state=42,   # Ensures the split is reproducible
    stratify=labels    # Maintain class balance in splits
)
print(f"Training set: {len(paths_train)} images. Test set: {len(paths_test)} images.")

## 5. HELPER FUNCTIONS TO LOAD .MAT FEATURES
def get_pca_descriptor_path(img_path, desc_dir):
    """
    Maps an original image path to its corresponding .mat descriptor path.
    Example:
    img_path = '.../Images/1_1_s.bmp'
    desc_dir = '.../lbp_pca'
    Returns: '.../lbp_pca/1_1_s.mat'
    """
    filename = os.path.basename(img_path)
    base_name = os.path.splitext(filename)[0]  # '1_1_s'
    return os.path.join(desc_dir, base_name + ".mat")

def load_pca_features_from_paths(paths, desc_dir):
    """
    Loops through a list of image paths (e.g., paths_train),
    finds the corresponding .mat file for each, and loads the
    feature vector from it.
    """
    features = []
    # tqdm wraps the loop to show a nice progress bar
    for img_path in tqdm(paths, desc="Loading PCA features"):
        desc_path = get_pca_descriptor_path(img_path, desc_dir)
        try:
            # Load the .mat file
            mat_data = sio.loadmat(desc_path)
            # Assumes the vector is stored under the key 'F'
            vec = mat_data['F']
            # Flatten to ensure it's a 1D vector and append
            features.append(vec.flatten())
        except FileNotFoundError:
            print(f"Warning: Descriptor file not found: {desc_path}")
        except Exception as e:
            print(f"Error loading {desc_path}: {e}")
            
    # Convert the list of 1D vectors into a 2D NumPy array (N_samples x N_features)
    return np.array(features)

## 6. LOAD X_train AND X_test FEATURE DATA
# Now we use the functions above to load the actual data
# for our training and testing sets.
X_train = load_pca_features_from_paths(paths_train, PCA_DESCRIPTOR_PATH)
X_test = load_pca_features_from_paths(paths_test, PCA_DESCRIPTOR_PATH)

print(f"Features loaded. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")

## 7. TRAIN SVM WITH GridSearchCV
# We use a Pipeline to chain a StandardScaler and the SVM.
# Why StandardScaler? Even though PCA was scaled, the resulting
# principal components themselves may have different variances.
# SVMs are sensitive to feature scales, so re-scaling the
# PCA components to have unit variance is good practice.

# Define the 'parameter grid' to search.
# These are hyperparameters for the 'rbf' kernel SVM.
svm_param_grid = {
    # 'svm__C': Regularization parameter.
    'svm__C': [1, 10, 100, 1000],
    # 'svm__gamma': Kernel coefficient.
    'svm__gamma': [0.1, 0.01, 0.001, 0.0001]
}

print("Running GridSearchCV for SVM...")
# The pipeline defines the steps:
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Step 1: Scale data
    ('svm', SVC(kernel='rbf'))     # Step 2: Classify with SVM
])

# GridSearchCV will automatically test all combinations of the
# param_grid using 5-fold cross-validation (cv=5).
# n_jobs=-1 uses all available CPU cores to speed up the search.
grid_search = GridSearchCV(pipeline, svm_param_grid, cv=5, verbose=2, n_jobs=-1)

start_time = time.time()
# Fit the grid search object to the training data
grid_search.fit(X_train, y_train)
print(f"GridSearchCV finished in {time.time() - start_time:.2f} seconds.")

## 8. SHOW FINAL RESULTS AND VISUALIZATIONS
print("\n===== FINAL RESULTS (with PCA) =====")
print(f"Descriptor: PCA (from {PCA_DESCRIPTOR_PATH})")
# 'grid_search.best_params_' holds the winning combination
print(f"Best SVM Params: {grid_search.best_params_}")
# 'grid_search.best_score_' is the avg cross-validation accuracy
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

# 'grid_search.score' uses the *best found model* to evaluate
# the held-back test set. This is the final performance metric.
test_accuracy = grid_search.score(X_test, y_test)
print(f"FINAL Test Set Accuracy: {test_accuracy:.4f}")

# Get predictions on the test set for the confusion matrix
y_pred = grid_search.predict(X_test)
class_names = grid_search.classes_


## 8a. Plot Confusion Matrix (Seaborn method)
print("Plotting confusion matrix...")

# Sort class names numerically 
# The labels are strings ('1', '10', '2', ...).
# We sort them numerically ('1', '2', ..., '10') for a cleaner plot.
try:
    class_names_int = [int(c) for c in class_names]
    class_names_int.sort()
    sorted_class_names = [str(c) for c in class_names_int]
except ValueError:
    # Fallback if labels are not numbers (e.g., 'cat', 'dog')
    sorted_class_names = sorted(class_names)

# --- Calculate the matrix ---
# We use the sorted_class_names list in the 'labels' param
# to ensure the matrix rows/columns follow this sorted order.
cm = confusion_matrix(y_test, y_pred, labels=sorted_class_names)

# --- Normalize the matrix ---
# Normalize the matrix to show percentages (from 0 to 1)
# We add a small epsilon to avoid division by zero if a class
# has no samples in the test set (which shouldn't happen with stratify, but is safe)
row_sums = cm.sum(axis=1)[:, np.newaxis]
cm_normalized = cm.astype('float') / (row_sums + 1e-9)


# --- Plot with Seaborn ---
plt.figure(figsize=(12, 10))
sns.heatmap(cm_normalized, 
            annot=True, fmt='.2f', cmap='Blues', 
            xticklabels=sorted_class_names, yticklabels=sorted_class_names,
            cbar_kws={'shrink': 0.8}) # Add shrink to help with layout

plt.title(f'Confusion Matrix (Row-Normalized)\nPCA Features')
plt.ylabel('True Category')
plt.xlabel('Predicted Category')

# --- Save the plot ---
cm_plot_filename = os.path.join(PCA_DESCRIPTOR_PATH, "svm_confusion_matrix.pdf")
try:
    # Note: We are NOT using plt.tight_layout() here,
    # as bbox_inches='tight' is usually sufficient and avoids the bug.
    plt.savefig(cm_plot_filename, format='pdf', bbox_inches='tight')
    print(f"Saved confusion matrix to {cm_plot_filename}")
except Exception as e:
    print(f"Warning: Could not save confusion matrix. Error: {e}")
# --- End Save ---

plt.show()


## 8b. Visualize Random Test Images
print("Visualizing random test images...")
# Pick 15 random indices from the test set
num_images_to_show = 15
sample_indices = random.sample(range(len(paths_test)), num_images_to_show)

fig, axes = plt.subplots(3, 5, figsize=(15, 10))
fig.suptitle("Test Image Predictions (PCA Features)", fontsize=16)

for i, ax in enumerate(axes.flatten()):
    if i >= len(sample_indices):
        ax.axis('off')  # Hide unused subplots
        continue
        
    # Get the index and the corresponding path/labels
    idx = sample_indices[i]
    img_path = paths_test[idx]
    true_label = y_test[idx]
    pred_label = y_pred[idx]

    # Load and display the image
    img = cv2.imread(img_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    ax.imshow(img_rgb)
    
    # Set the title, coloring it green for correct, red for incorrect
    title = f"True: {true_label}\nPred: {pred_label}"
    color = 'green' if true_label == pred_label else 'red'
    ax.set_title(title, color=color)
    ax.axis('off')
    
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

# --- Save the plot ---
pred_plot_filename = os.path.join(PCA_DESCRIPTOR_PATH, "svm_test_predictions.pdf")
try:
    plt.savefig(pred_plot_filename, format='pdf', bbox_inches='tight')
    print(f"Saved prediction visualizations to {pred_plot_filename}")
except Exception as e:
    print(f"Warning: Could not save predictions plot. Error: {e}")
# --- End Save ---

plt.show()